{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12716340,"sourceType":"datasetVersion","datasetId":8037192},{"sourceId":89928,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":75420,"modelId":100145},{"sourceId":90860,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":76172,"modelId":100857}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Please check the Live Notebook from here : https://www.kaggle.com/code/saurabhmaulekhi/parallel-dot-assignments/edit/run/255438169","metadata":{}},{"cell_type":"markdown","source":"# Installing sam2","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/facebookresearch/segment-anything-2.gitzzz","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:41:06.069633Z","iopub.execute_input":"2025-08-12T10:41:06.069876Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/facebookresearch/segment-anything-2.gitzzz\n  Cloning https://github.com/facebookresearch/segment-anything-2.gitzzz to /tmp/pip-req-build-w9rtf0_e\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything-2.gitzzz /tmp/pip-req-build-w9rtf0_e\nUsername for 'https://github.com': ","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Imporing Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\nfrom sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\nfrom sam2.build_sam import build_sam2_video_predictor\n\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os,glob,shutil\nimport matplotlib.patches as patches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"objects_list = [\"can_chowder\",\"can_soymilk\", \"can_tomatosoup\", \"carton_oj\", \"carton_soymilk\", \n                \"diet_coke\", \"hc_potroastsoup\", \"juicebox\", \"rice_tuscan\", \"ricepilaf\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(objects_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numbers = [str(i).zfill(6) for i in range(1, 51)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"initial_dir = \"/kaggle/input/cmu10-3d/CMU10_3D/data_2D/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"can_chowder_images_list = [initial_dir + objects_list[0] + '_' + num + '.jpg' for num in numbers  ]\ncan_soymilk_images_list = [initial_dir + objects_list[1] + '_' + num + '.jpg' for num in numbers  ]\ncan_tomatosoup_images_list = [initial_dir + objects_list[2] + '_' + num + '.jpg' for num in numbers  ]\ncarton_oj_images_list = [initial_dir + objects_list[3] + '_' + num + '.jpg' for num in numbers  ]\ncarton_soymilk_images_list = [initial_dir + objects_list[4] + '_' + num + '.jpg' for num in numbers  ]\ndiet_coke_images_list = [initial_dir + objects_list[5] + '_' + num + '.jpg' for num in numbers  ]\nhc_potroastsoup_images_list = [initial_dir + objects_list[6] + '_' + num + '.jpg' for num in numbers  ]\njuicebox_images_list = [initial_dir + objects_list[7] + '_' + num + '.jpg' for num in numbers  ]\nrice_tuscan_images_list = [initial_dir + objects_list[8] + '_' + num + '.jpg' for num in numbers  ]\nricepilaf_images_list = [initial_dir + objects_list[9] + '_' + num + '.jpg' for num in numbers  ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Nested list containg each and every image files\n\nObjects_images_list = [can_chowder_images_list,\n                        can_soymilk_images_list,\n                        can_tomatosoup_images_list,\n                        carton_oj_images_list,\n                        carton_soymilk_images_list,\n                        diet_coke_images_list,\n                        hc_potroastsoup_images_list,\n                        juicebox_images_list,\n                        rice_tuscan_images_list,\n                        ricepilaf_images_list]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filtering the mask files.\n# Appending masked images from a image to a single list \n\nfiles_list = os.listdir(initial_dir)\n\nmask_file_list = []\n\nfor i in files_list:\n    if \"gt\" in i:\n        mask_file_list.append(initial_dir + i)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\n# Group by object name and main number\ngrouped = defaultdict(lambda: defaultdict(list))\n\nfor f in mask_file_list:\n    parts = f.split(\"_\")\n    obj_name = \"_\".join(parts[:-3])       # everything before the main number\n    main_num = parts[-3]                  # the 6-digit number\n    grouped[obj_name][main_num].append(f)\n\n# Convert to nested, nested list\nresult = [\n    [grouped[obj][num] for num in sorted(grouped[obj])]\n    for obj in sorted(grouped)\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"can_chowder_mask_list = result[0]\ncan_soymilk_mask_list = result[1]\ncan_tomatosoup_mask_list = result[2]\ncarton_oj_mask_list = result[3]\ncarton_soymilk_mask_list = result[4]\ndiet_coke_mask_list = result[5]\nhc_potroastsoup_mask_list = result[6]\njuicebox_mask_list = result[7]\nrice_tuscan_mask_list = result[8]\nricepilaf_mask_list = result[9]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List containg all the mask images path\n\nObjects_mask_list = [can_chowder_mask_list,\n                        can_soymilk_mask_list,\n                        can_tomatosoup_mask_list,\n                        carton_oj_mask_list,\n                        carton_soymilk_mask_list,\n                        diet_coke_mask_list,\n                        hc_potroastsoup_mask_list,\n                        juicebox_mask_list,\n                        rice_tuscan_mask_list,\n                        ricepilaf_mask_list]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load SAM2 model\nmodel_cfg = \"sam2_hiera_b+.yaml\"\ncheckpoint = \"/kaggle/input/segment-anything-2/pytorch/sam2-hiera-base-plus/1/sam2_hiera_base_plus.pt\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\npredictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint, device=device))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_BBox(obj_image, mask_image):\n    \"\"\"\n    obj_image: image file path\n    mask_image : mask image file path\n\n    return : BBox of given mask , [xmin, xmax, ymin, ymax ]\n    \"\"\"\n    image = cv2.imread(obj_image)\n    predictor.set_image(image)\n    \n    # Load mask\n    mask = cv2.imread(mask_image, cv2.IMREAD_GRAYSCALE)\n    ys, xs = np.where(mask > 0)\n\n    # If no object pixels found → return zeros\n    if xs.size == 0 or ys.size == 0:\n        return [0, 0, 0, 0]\n    \n    xmin, xmax = xs.min(), xs.max()\n    ymin, ymax = ys.min(), ys.max()\n\n    return [xmin, xmax, ymin, ymax]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bounding Boxes of each object from an initial image and mask\n\nBBox_can_chowder =[(find_BBox(can_chowder_images_list[0], can_chowder_mask_list[0][0]), 0 )]\n\nBBox_can_soymilk = [(find_BBox(can_soymilk_images_list[0], can_soymilk_mask_list[1][0]), 1 )]\n\nBBox_can_tomatosoup = [(find_BBox(can_tomatosoup_images_list[0], can_tomatosoup_mask_list[2][0]), 2 )]\n\nBBox_carton_oj = [(find_BBox(carton_oj_images_list[0], carton_oj_mask_list[3][0]), 3 )]\n\nBBox_carton_soymilk = [(find_BBox(carton_soymilk_images_list[0], carton_soymilk_mask_list[4][0]), 4 )]\n\nBBox_diet_coke = [(find_BBox(diet_coke_images_list[0], diet_coke_mask_list[5][0]), 5 )]\n\nBBox_hc_potroastsoup = [(find_BBox(hc_potroastsoup_images_list[0], hc_potroastsoup_mask_list[6][0]), 6 )]\n\nBBox_juicebox = [(find_BBox(juicebox_images_list[0], juicebox_mask_list[7][0]), 7 )]\n\nBBox_rice_tuscan = [(find_BBox(rice_tuscan_images_list[32], rice_tuscan_mask_list[9][0]), 8 )]\n\nBBox_ricepilaf = [(find_BBox(ricepilaf_images_list[11], ricepilaf_mask_list[9][0]), 9 )]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# list of BBox of initial image and objects\n\nObjests_BBox_list = [ BBox_can_chowder,\n                    BBox_can_soymilk,\n                    BBox_can_tomatosoup,\n                    BBox_carton_oj,\n                    BBox_carton_soymilk,\n                    BBox_diet_coke,\n                    BBox_hc_potroastsoup,\n                    BBox_juicebox,\n                    BBox_rice_tuscan,\n                    BBox_ricepilaf,\n                    ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Segementation","metadata":{}},{"cell_type":"code","source":"checkpoint = \"/kaggle/input/sam2_hiera_tiny.pt/pytorch/default/1/sam2_hiera_tiny.pt\"\nmodel_cfg = \"sam2_hiera_t.yaml\"\n\n# checkpoint = \"/kaggle/input/segment-anything-2.1/pytorch/sam2.1-hiera-base-plus/1/sam2.1_hiera_base_plus.pt\"\n# model_cfg = \"/kaggle/input/segment-anything-2.1/pytorch/sam2.1-hiera-base-plus/1sam2.1_hiera_b.yaml\"\n\npredictor_prompt = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))\nsam2 = build_sam2(model_cfg, checkpoint, device='cuda', apply_postprocessing=False)\nmask_generator = SAM2AutomaticMaskGenerator(sam2)\npredictor_vid = build_sam2_video_predictor(model_cfg, checkpoint,device='cuda')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tempfolder = \"/kaggle/working/tempdir\"\nmask_dir = \"/kaggle/working/mask_dir\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_if_not_exists(dirname):\n    if not os.path.exists(dirname):\n        os.mkdir(dirname)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"create_if_not_exists(mask_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cleardir(tempfolder):\n    filepaths = glob.glob(tempfolder+\"/*\")\n    for filepath in filepaths:\n        os.unlink(filepath)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_mask(mask, ax, obj_id=None, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        cmap = plt.get_cmap(\"tab10\")\n        cmap_idx = 0 if obj_id is None else obj_id\n        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_points(coords, labels, ax, marker_size=200):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0,)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def track_item_boxes(imgpath1,imgpath2,img1boxclasslist,visualize=True):\n    # imgpath1 :: Image where object is known\n    # imgpath2 :: Image where object is to be tracked\n    # img1boxclasslist :: [ ([xmin,xmax,ymin,ymax],objectnumint) ,....] for all objects in im\n\n    create_if_not_exists(tempfolder)\n    cleardir(tempfolder)\n\n    shutil.copy(imgpath1,tempfolder+\"/00000.jpg\")\n    shutil.copy(imgpath2,tempfolder+\"/00001.jpg\")\n    \n    #inference_state stores context for tracking across frames.\n    inference_state = predictor_vid.init_state(video_path=\"/kaggle/working/tempdir\")\n\n    #Resets any existing tracker memory to a clean state before adding annotations.\n    predictor_vid.reset_state(inference_state)\n\n    # Index of the frame that contains the known object location.\n    # Here 0 means the first frame (00000.jpg).\n    ann_frame_idx = 0\n\n    # Loops through all (bounding box, object_id) pairs in the list.\n    # Unpacks the box coordinates into variables xmin, xmax, ymin, ymax.\n    # objectnumint is the integer ID for that object.\n    for img1boxclass in img1boxclasslist:\n    \n        ([xmin,xmax,ymin,ymax],objectnumint) = img1boxclass\n        box = np.array([xmin, ymin, xmax, ymax], dtype=np.float32)\n        _, out_obj_ids, out_mask_logits = predictor_vid.add_new_points_or_box(\n        inference_state=inference_state,\n        frame_idx=ann_frame_idx,\n        obj_id=objectnumint,\n        box=box,\n        )\n        \n    video_segments = {} # video_segments contains the per-frame segmentation results. ex: {frame_index: {object_id: binary_mask_array}}\n        \n    # Calls propagate_in_video(inference_state) which:\n    # Runs the tracker over all frames. Yields (frame_index, object_ids, mask_logits) for each frame.\n    for out_frame_idx, out_obj_ids, out_mask_logits in predictor_vid.propagate_in_video(inference_state):\n\n        # For each frame:\n        # Creates a dictionary mapping each object_id → binary mask array.\n        # (out_mask_logits[i] > 0.0) turns the mask logits into a boolean mask (True = object present).\n        # .cpu().numpy() moves it from GPU to CPU and converts to a NumPy array\n        video_segments[out_frame_idx] = {\n        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n        \n            for i, out_obj_id in enumerate(out_obj_ids)\n        }\n    \n    return video_segments","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# listy containg index for each object\n\nObjects_idx = [ x for x in range(10)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## to hide the output complition bars by SAM2\nimport sys, os, builtins\norig_out, orig_err, orig_print = sys.stdout, sys.stderr, print\nsys.stdout = sys.stderr = open(os.devnull, 'w')\nbuiltins.print = lambda *a, **k: orig_print(*a, file=orig_out, **k)\n#=====================================================#\n\nfor object_images_list , object_BBox, Object_idx in zip(Objects_images_list, Objests_BBox_list, Objects_idx):\n\n    print('['+objects_list[Object_idx]+ ']', end=\"\")\n    \n    for object_num in range(50): # Because each type of object has 50 images\n        \n        result = track_item_boxes(object_images_list[0],     # image segemntation\n                                  object_images_list[object_num],\n                                  object_BBox\n                                 )\n        \n        mask = result[1][Object_idx]  # from our SAM2 output\n        mask = mask.squeeze()  # remove extra dims\n        mask = (mask.astype(np.uint8)) * 255  # scale to 0-255 , it's binary 0/1\n        \n\n        new_masked_img_path = mask_dir + \"/\"+objects_list[Object_idx] + '_' + numbers[object_num]+ \"_1_gt.png\"\n        \n        cv2.imwrite(new_masked_img_path, mask) # saving new segmented mask image\n        \n        print(\"-\", end=\"\")\n\n    print(\">\",\"✅\")\n\n\nsys.stdout, sys.stderr, builtins.print = orig_out, orig_err, orig_print # Reseting outputs, so that other cells could show complition bar ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss Calculation","metadata":{}},{"cell_type":"code","source":"# Flat list are non nested list that contain just the images path \n\nFlat_Objects_images_list = [item for sublist in Objects_images_list for item in sublist ]\n\nFlat_Objects_mask_list = [inner[0] for outer in Objects_mask_list for inner in outer]\n\nPredicted_mask_list = [mask_dir+\"/\"+ x for x in os.listdir(mask_dir)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sorting list\n\nObjects_images_list.sort()\nObjects_mask_list.sort()\nPredicted_mask_list.sort()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculating the BBox from both truth anfd predicted segmentations and storing them in a list\n\nfrom tqdm import tqdm\n\nTruth_BBox = [ find_BBox(img, mask) for img, mask in tqdm(zip(Flat_Objects_images_list, Flat_Objects_mask_list))] \n\nPredicted_BBox = [ find_BBox(img, mask) for img, mask in tqdm(zip(Flat_Objects_images_list, Predicted_mask_list))]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(Truth_BBox[0])\nprint(Predicted_BBox[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bbox_iou_Error(truthBBox, predictedBBox):\n    \"\"\"\n    truthBBox : BBox of from true mask\n    predictedBBox : BBox from predicted mask\n\n    return : Error of Inter section over union between truthBBox and predictedBBox \n    \"\"\"\n    \n    # Unpack\n    txmin, txmax, tymin, tymax = truthBBox\n    pxmin, pxmax, pymin, pymax = predictedBBox\n\n    # Intersection coordinates\n    ixmin = max(txmin, pxmin)\n    iymin = max(tymin, pymin)\n    ixmax = min(txmax, pxmax)\n    iymax = min(tymax, pymax)\n\n    # Intersection area\n    inter_width = max(0, ixmax - ixmin)\n    inter_height = max(0, iymax - iymin)\n    inter_area = inter_width * inter_height\n\n    # Areas of truth and predicted boxes\n    truth_area = (txmax - txmin) * (tymax - tymin)\n    pred_area = (pxmax - pxmin) * (pymax - pymin)\n\n    # Union area\n    union_area = truth_area + pred_area - inter_area\n    \n    # IoU\n    return (1 - (inter_area / union_area if union_area > 0 else 0.0))\n\nprint(bbox_iou_Error([371, 464, 150, 290],[374, 463, 154, 290]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of Error/loss between corresponding masks\n\nBBox_Errors_list = [bbox_iou_Error(truthBBox, predictedBBox) for  truthBBox, predictedBBox in zip(Truth_BBox, Predicted_BBox)] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Getting list of images\n\nObjects_image_names = [objects_list[x]+ '_' + num + \".png\"  for x in range(10) for num in numbers ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission file","metadata":{}},{"cell_type":"code","source":"import pandas  as pd\n\n\ndf = pd.DataFrame({\n    'Image Name': Objects_image_names,\n    'Error Between the Orignal and predicted Bounding Box': BBox_Errors_list\n})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"parallel_dots_assignment_submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}